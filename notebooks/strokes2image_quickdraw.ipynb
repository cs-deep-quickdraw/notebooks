{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quickdraw_generate.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9BAgeDMSDm5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bbe3d28-cade-40d0-93d4-394c6e1a5ee0"
      },
      "source": [
        "import struct\n",
        "from struct import unpack\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device = {}'.format(device))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device = cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Rr7QTaSINl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "BASE_SIZE = 256\n",
        "\n",
        "# Helper from: https://github.com/googlecreativelab/quickdraw-dataset/blob/master/examples/binary_file_parser.py\n",
        "def unpack_drawing(file_handle, size, lw):\n",
        "    # Skip key_id: 8, countrycode: 2, recognized: 1, timestamp: 4 = 15\n",
        "    file_handle.read(15)\n",
        "    n_strokes, = unpack('H', file_handle.read(2))\n",
        "    idx = 0\n",
        "\n",
        "    N = 0\n",
        "    strokes = []\n",
        "    for i in range(n_strokes):\n",
        "      n_points, = unpack('H', file_handle.read(2))\n",
        "      N += n_points\n",
        "      fmt = str(n_points) + 'B'\n",
        "      x = unpack(fmt, file_handle.read(n_points))\n",
        "      y = unpack(fmt, file_handle.read(n_points))\n",
        "      strokes.append((x, y))\n",
        "\n",
        "    image = np.zeros((BASE_SIZE, BASE_SIZE, 3), dtype=np.uint8)\n",
        "    \n",
        "    # Build the image by drawing the strokes\n",
        "    for t, (x, y) in enumerate(strokes):\n",
        "        for i in range(len(x) - 1):\n",
        "          p1 = (x[i], y[i])\n",
        "          p2 = (x[i+1], y[i+1])\n",
        "          cv2.line(image, p1, p2, (255, 255, 255), lw)\n",
        "\n",
        "    return image if size == BASE_SIZE else cv2.resize(image, (size, size))\n",
        "\n",
        "def skip_drawing(file_handle):\n",
        "    # Skip key_id: 8, countrycode: 2, recognized: 1, timestamp: 4 = 15\n",
        "    file_handle.read(15)\n",
        "    n_strokes, = unpack('H', file_handle.read(2))\n",
        "    idx = 0\n",
        "\n",
        "    N = 0\n",
        "    strokes = []\n",
        "    for i in range(n_strokes):\n",
        "      n_points, = unpack('H', file_handle.read(2))\n",
        "      file_handle.read(2 * n_points)\n",
        "\n",
        "\n",
        "def unpack_drawings(to_skip, filename, size=64, lw=6):\n",
        "    with open(filename, 'rb') as f:\n",
        "        for _ in range(to_skip):\n",
        "          skip_drawing(f)\n",
        "        while True:\n",
        "            try:\n",
        "                yield unpack_drawing(f, size, lw)\n",
        "            except struct.error:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia0ZqFBPTZXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "urllib.request.urlretrieve('https://raw.githubusercontent.com/cs-deep-quickdraw/notebooks/master/100_classes.txt', '100_classes.txt')\n",
        "\n",
        "# Create data dir\n",
        "Path(\"./data\").mkdir(exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaF1Wg5vTb70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"100_classes.txt\",\"r\")\n",
        "# And for reading use\n",
        "classes = [cls.strip() for cls in f.readlines()]\n",
        "f.close()\n",
        "\n",
        "def download(classes):\n",
        "  base = 'https://storage.googleapis.com/quickdraw_dataset/full/binary/'\n",
        "  for i, c in enumerate(classes):\n",
        "    cls_url = c.replace('_', '%20')\n",
        "    path = base+cls_url+'.bin'\n",
        "    print((1+i)/len(classes), c, path)\n",
        "    urllib.request.urlretrieve(path, 'data/'+c+'.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxd9F-nHTdc5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65eff0f9-41e7-41f3-d716-0f723abac0bc"
      },
      "source": [
        "download(classes)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01 drums https://storage.googleapis.com/quickdraw_dataset/full/binary/drums.bin\n",
            "0.02 sun https://storage.googleapis.com/quickdraw_dataset/full/binary/sun.bin\n",
            "0.03 laptop https://storage.googleapis.com/quickdraw_dataset/full/binary/laptop.bin\n",
            "0.04 anvil https://storage.googleapis.com/quickdraw_dataset/full/binary/anvil.bin\n",
            "0.05 baseball_bat https://storage.googleapis.com/quickdraw_dataset/full/binary/baseball%20bat.bin\n",
            "0.06 ladder https://storage.googleapis.com/quickdraw_dataset/full/binary/ladder.bin\n",
            "0.07 eyeglasses https://storage.googleapis.com/quickdraw_dataset/full/binary/eyeglasses.bin\n",
            "0.08 grapes https://storage.googleapis.com/quickdraw_dataset/full/binary/grapes.bin\n",
            "0.09 book https://storage.googleapis.com/quickdraw_dataset/full/binary/book.bin\n",
            "0.1 dumbbell https://storage.googleapis.com/quickdraw_dataset/full/binary/dumbbell.bin\n",
            "0.11 traffic_light https://storage.googleapis.com/quickdraw_dataset/full/binary/traffic%20light.bin\n",
            "0.12 wristwatch https://storage.googleapis.com/quickdraw_dataset/full/binary/wristwatch.bin\n",
            "0.13 wheel https://storage.googleapis.com/quickdraw_dataset/full/binary/wheel.bin\n",
            "0.14 shovel https://storage.googleapis.com/quickdraw_dataset/full/binary/shovel.bin\n",
            "0.15 bread https://storage.googleapis.com/quickdraw_dataset/full/binary/bread.bin\n",
            "0.16 table https://storage.googleapis.com/quickdraw_dataset/full/binary/table.bin\n",
            "0.17 tennis_racquet https://storage.googleapis.com/quickdraw_dataset/full/binary/tennis%20racquet.bin\n",
            "0.18 cloud https://storage.googleapis.com/quickdraw_dataset/full/binary/cloud.bin\n",
            "0.19 chair https://storage.googleapis.com/quickdraw_dataset/full/binary/chair.bin\n",
            "0.2 headphones https://storage.googleapis.com/quickdraw_dataset/full/binary/headphones.bin\n",
            "0.21 face https://storage.googleapis.com/quickdraw_dataset/full/binary/face.bin\n",
            "0.22 eye https://storage.googleapis.com/quickdraw_dataset/full/binary/eye.bin\n",
            "0.23 airplane https://storage.googleapis.com/quickdraw_dataset/full/binary/airplane.bin\n",
            "0.24 snake https://storage.googleapis.com/quickdraw_dataset/full/binary/snake.bin\n",
            "0.25 lollipop https://storage.googleapis.com/quickdraw_dataset/full/binary/lollipop.bin\n",
            "0.26 power_outlet https://storage.googleapis.com/quickdraw_dataset/full/binary/power%20outlet.bin\n",
            "0.27 pants https://storage.googleapis.com/quickdraw_dataset/full/binary/pants.bin\n",
            "0.28 mushroom https://storage.googleapis.com/quickdraw_dataset/full/binary/mushroom.bin\n",
            "0.29 star https://storage.googleapis.com/quickdraw_dataset/full/binary/star.bin\n",
            "0.3 sword https://storage.googleapis.com/quickdraw_dataset/full/binary/sword.bin\n",
            "0.31 clock https://storage.googleapis.com/quickdraw_dataset/full/binary/clock.bin\n",
            "0.32 hot_dog https://storage.googleapis.com/quickdraw_dataset/full/binary/hot%20dog.bin\n",
            "0.33 syringe https://storage.googleapis.com/quickdraw_dataset/full/binary/syringe.bin\n",
            "0.34 stop_sign https://storage.googleapis.com/quickdraw_dataset/full/binary/stop%20sign.bin\n",
            "0.35 mountain https://storage.googleapis.com/quickdraw_dataset/full/binary/mountain.bin\n",
            "0.36 smiley_face https://storage.googleapis.com/quickdraw_dataset/full/binary/smiley%20face.bin\n",
            "0.37 apple https://storage.googleapis.com/quickdraw_dataset/full/binary/apple.bin\n",
            "0.38 bed https://storage.googleapis.com/quickdraw_dataset/full/binary/bed.bin\n",
            "0.39 shorts https://storage.googleapis.com/quickdraw_dataset/full/binary/shorts.bin\n",
            "0.4 broom https://storage.googleapis.com/quickdraw_dataset/full/binary/broom.bin\n",
            "0.41 diving_board https://storage.googleapis.com/quickdraw_dataset/full/binary/diving%20board.bin\n",
            "0.42 flower https://storage.googleapis.com/quickdraw_dataset/full/binary/flower.bin\n",
            "0.43 spider https://storage.googleapis.com/quickdraw_dataset/full/binary/spider.bin\n",
            "0.44 cell_phone https://storage.googleapis.com/quickdraw_dataset/full/binary/cell%20phone.bin\n",
            "0.45 car https://storage.googleapis.com/quickdraw_dataset/full/binary/car.bin\n",
            "0.46 camera https://storage.googleapis.com/quickdraw_dataset/full/binary/camera.bin\n",
            "0.47 tree https://storage.googleapis.com/quickdraw_dataset/full/binary/tree.bin\n",
            "0.48 square https://storage.googleapis.com/quickdraw_dataset/full/binary/square.bin\n",
            "0.49 moon https://storage.googleapis.com/quickdraw_dataset/full/binary/moon.bin\n",
            "0.5 radio https://storage.googleapis.com/quickdraw_dataset/full/binary/radio.bin\n",
            "0.51 hat https://storage.googleapis.com/quickdraw_dataset/full/binary/hat.bin\n",
            "0.52 pizza https://storage.googleapis.com/quickdraw_dataset/full/binary/pizza.bin\n",
            "0.53 axe https://storage.googleapis.com/quickdraw_dataset/full/binary/axe.bin\n",
            "0.54 door https://storage.googleapis.com/quickdraw_dataset/full/binary/door.bin\n",
            "0.55 tent https://storage.googleapis.com/quickdraw_dataset/full/binary/tent.bin\n",
            "0.56 umbrella https://storage.googleapis.com/quickdraw_dataset/full/binary/umbrella.bin\n",
            "0.57 line https://storage.googleapis.com/quickdraw_dataset/full/binary/line.bin\n",
            "0.58 cup https://storage.googleapis.com/quickdraw_dataset/full/binary/cup.bin\n",
            "0.59 fan https://storage.googleapis.com/quickdraw_dataset/full/binary/fan.bin\n",
            "0.6 triangle https://storage.googleapis.com/quickdraw_dataset/full/binary/triangle.bin\n",
            "0.61 basketball https://storage.googleapis.com/quickdraw_dataset/full/binary/basketball.bin\n",
            "0.62 pillow https://storage.googleapis.com/quickdraw_dataset/full/binary/pillow.bin\n",
            "0.63 scissors https://storage.googleapis.com/quickdraw_dataset/full/binary/scissors.bin\n",
            "0.64 t-shirt https://storage.googleapis.com/quickdraw_dataset/full/binary/t-shirt.bin\n",
            "0.65 tooth https://storage.googleapis.com/quickdraw_dataset/full/binary/tooth.bin\n",
            "0.66 alarm_clock https://storage.googleapis.com/quickdraw_dataset/full/binary/alarm%20clock.bin\n",
            "0.67 paper_clip https://storage.googleapis.com/quickdraw_dataset/full/binary/paper%20clip.bin\n",
            "0.68 spoon https://storage.googleapis.com/quickdraw_dataset/full/binary/spoon.bin\n",
            "0.69 microphone https://storage.googleapis.com/quickdraw_dataset/full/binary/microphone.bin\n",
            "0.7 candle https://storage.googleapis.com/quickdraw_dataset/full/binary/candle.bin\n",
            "0.71 pencil https://storage.googleapis.com/quickdraw_dataset/full/binary/pencil.bin\n",
            "0.72 envelope https://storage.googleapis.com/quickdraw_dataset/full/binary/envelope.bin\n",
            "0.73 saw https://storage.googleapis.com/quickdraw_dataset/full/binary/saw.bin\n",
            "0.74 frying_pan https://storage.googleapis.com/quickdraw_dataset/full/binary/frying%20pan.bin\n",
            "0.75 screwdriver https://storage.googleapis.com/quickdraw_dataset/full/binary/screwdriver.bin\n",
            "0.76 helmet https://storage.googleapis.com/quickdraw_dataset/full/binary/helmet.bin\n",
            "0.77 bridge https://storage.googleapis.com/quickdraw_dataset/full/binary/bridge.bin\n",
            "0.78 light_bulb https://storage.googleapis.com/quickdraw_dataset/full/binary/light%20bulb.bin\n",
            "0.79 ceiling_fan https://storage.googleapis.com/quickdraw_dataset/full/binary/ceiling%20fan.bin\n",
            "0.8 key https://storage.googleapis.com/quickdraw_dataset/full/binary/key.bin\n",
            "0.81 donut https://storage.googleapis.com/quickdraw_dataset/full/binary/donut.bin\n",
            "0.82 bird https://storage.googleapis.com/quickdraw_dataset/full/binary/bird.bin\n",
            "0.83 circle https://storage.googleapis.com/quickdraw_dataset/full/binary/circle.bin\n",
            "0.84 beard https://storage.googleapis.com/quickdraw_dataset/full/binary/beard.bin\n",
            "0.85 coffee_cup https://storage.googleapis.com/quickdraw_dataset/full/binary/coffee%20cup.bin\n",
            "0.86 butterfly https://storage.googleapis.com/quickdraw_dataset/full/binary/butterfly.bin\n",
            "0.87 bench https://storage.googleapis.com/quickdraw_dataset/full/binary/bench.bin\n",
            "0.88 rifle https://storage.googleapis.com/quickdraw_dataset/full/binary/rifle.bin\n",
            "0.89 cat https://storage.googleapis.com/quickdraw_dataset/full/binary/cat.bin\n",
            "0.9 sock https://storage.googleapis.com/quickdraw_dataset/full/binary/sock.bin\n",
            "0.91 ice_cream https://storage.googleapis.com/quickdraw_dataset/full/binary/ice%20cream.bin\n",
            "0.92 moustache https://storage.googleapis.com/quickdraw_dataset/full/binary/moustache.bin\n",
            "0.93 suitcase https://storage.googleapis.com/quickdraw_dataset/full/binary/suitcase.bin\n",
            "0.94 hammer https://storage.googleapis.com/quickdraw_dataset/full/binary/hammer.bin\n",
            "0.95 rainbow https://storage.googleapis.com/quickdraw_dataset/full/binary/rainbow.bin\n",
            "0.96 knife https://storage.googleapis.com/quickdraw_dataset/full/binary/knife.bin\n",
            "0.97 cookie https://storage.googleapis.com/quickdraw_dataset/full/binary/cookie.bin\n",
            "0.98 baseball https://storage.googleapis.com/quickdraw_dataset/full/binary/baseball.bin\n",
            "0.99 lightning https://storage.googleapis.com/quickdraw_dataset/full/binary/lightning.bin\n",
            "1.0 bicycle https://storage.googleapis.com/quickdraw_dataset/full/binary/bicycle.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ6qOruLTgTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a603bcf-aff7-4287-a801-47c62f8243bc"
      },
      "source": [
        "import os\n",
        "\n",
        "print(len(os.listdir('data')))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncndZocFTiPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "62510593-b82b-41a5-da46-354eb07a158d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "drum_iterator = unpack_drawings(0, \"data/drums.bin\")\n",
        "plt.imshow(next(drum_iterator), cmap='gray')\n",
        "\n",
        "arr = []\n",
        "\n",
        "arr.append(next(drum_iterator))\n",
        "arr.append(next(drum_iterator))\n",
        "\n",
        "print(np.array(arr).shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 64, 64, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1snxmUDaTyd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "mobilenet_preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class DrawDataset(Dataset):\n",
        "    def __init__(self, X, Y, preprocess):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        return (self.preprocess(self.X[idx]), self.Y[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRVigCmUWVnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Config:\n",
        "batch_size = 256\n",
        "learning_rate = 0.001\n",
        "\n",
        "img_size = 64\n",
        "train_classes = classes[:]\n",
        "\n",
        "\n",
        "N_train = 8000\n",
        "N_val = N_train // 5\n",
        "N_test = 0\n",
        "# N_test = N_val\n",
        "N_test_reserved = 20000\n",
        "\n",
        "n_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHSr0PqNVbSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import islice\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def extract_dataset(samples_train, samples_val, samples_test, test_reserved, classes, preprocess, img_size=64):\n",
        "  X_train = []\n",
        "  X_val = []\n",
        "  X_test = []\n",
        "  y_train = []\n",
        "  y_val = []\n",
        "  y_test = []\n",
        "\n",
        "  for c, cls in enumerate(classes):\n",
        "    drawings = unpack_drawings(test_reserved - samples_train, 'data/' + cls + '.bin', size=img_size)\n",
        "\n",
        "    # TODO: better way of doing this\n",
        "    for _ in range(samples_train):\n",
        "      X_train.append(next(drawings))\n",
        "      y_train.append(c)\n",
        "\n",
        "    for _ in range(samples_val):\n",
        "      X_val.append(next(drawings))\n",
        "      y_val.append(c)\n",
        "\n",
        "    for _ in range(samples_test):\n",
        "      X_test.append(next(drawings))\n",
        "      y_test.append(c)\n",
        "  \n",
        "    \n",
        "    print(f\"\\rdone extracting class: {cls}: {1 + c} / {len(classes)}\", end='')\n",
        "\n",
        "    drawings.close()\n",
        "    \n",
        "\n",
        "  def norm(X):\n",
        "    return np.array(X)\n",
        "\n",
        "  X_train = norm(X_train)\n",
        "  X_val = norm(X_val)\n",
        "  X_test = norm(X_test)\n",
        "  print(\"training shape\", X_train.shape)\n",
        "  print(\"validation shape\", X_val.shape)\n",
        "  print(\"testing shape\", X_test.shape)\n",
        "  print(\"classes\", len(classes))\n",
        "\n",
        "  return (\n",
        "      DrawDataset(X_train, torch.LongTensor(y_train), preprocess), \n",
        "      DrawDataset(X_val, torch.LongTensor(y_val), preprocess),\n",
        "      DrawDataset(X_test, torch.LongTensor(y_test), preprocess),\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQfqEQv9WA1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, loader):\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for i, (img, label) in enumerate(loader):\n",
        "      img = img.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      out = model(img)\n",
        "\n",
        "      _, pred = torch.max(out.data, 1)\n",
        "\n",
        "      total += label.size(0)\n",
        "      correct += (pred == label).sum().item()\n",
        "\n",
        "    return 100. * correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOc7i8AxWKFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "def train_model(model, opt, loss_fn, loader, v_loader, n_epochs):\n",
        "\n",
        "  best_acc, best_model = 0, None\n",
        "  losses, accs = [], []\n",
        "  for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    epoch_losses = []\n",
        "    for i, (img, lab) in enumerate(loader):\n",
        "      print(f\"\\rbatch: {i}, current loss: {np.mean(epoch_losses) if epoch_losses else 'NaN'}\", end='')\n",
        "      img = img.to(device)\n",
        "      lab = lab.to(device)\n",
        "\n",
        "      out = model(img)\n",
        "\n",
        "      loss = loss_fn(out, lab)\n",
        "\n",
        "      opt.zero_grad()\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      epoch_losses.append(loss.item())\n",
        "\n",
        "    print(\"\\rEvaluating model on validation dataset...\", end='')\n",
        "    val_acc = evaluate_model(model, v_loader)\n",
        "    mean_loss = np.mean(epoch_losses)\n",
        "\n",
        "    losses.append(mean_loss)\n",
        "    accs.append(val_acc)\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "      best_acc = val_acc\n",
        "      best_model = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model, f\"lstm_epoch_{epoch}_acc_{val_acc}.model\")\n",
        "\n",
        "    print(f\"\\rEpoch: {epoch+1}/{n_epochs}, loss: {mean_loss}, validation accuracy: {val_acc}% took: {time.time() - start} seconds\")\n",
        "\n",
        "  print(f\"Training ended after {n_epochs} ! Best validation accuracy: {best_acc}%\")\n",
        " \n",
        "  return best_model, losses, accs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOISl2Puc_cE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "99f9eca1-1879-4700-d2b5-d5fd78456ebf"
      },
      "source": [
        "train_dataset, val_dataset, test_dataset = extract_dataset(N_train, N_val, N_test, N_test_reserved, train_classes, mobilenet_preprocess, img_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done extracting class: bicycle: 100 / 100training shape (800000, 64, 64, 3)\n",
            "validation shape (160000, 64, 64, 3)\n",
            "testing shape (160000, 64, 64, 3)\n",
            "classes 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x6yvVMfWoan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "315agUIIYE2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(\n",
        "    in_features=model.classifier[1].in_features,\n",
        "    out_features=len(train_classes), \n",
        "    bias=True\n",
        ")\n",
        "\n",
        "model.to(device) # puts model on GPU / CPU\n",
        "model.train()\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb6LBnKQc1tZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0beecfa6-0aa1-4560-9cf2-3fee318a1f49"
      },
      "source": [
        "best_model, losses, accs = train_model(model, optimizer, loss_function, train_loader, val_loader, n_epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch: 1600, current loss: 0.853685634471476"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GUld_sagt1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}