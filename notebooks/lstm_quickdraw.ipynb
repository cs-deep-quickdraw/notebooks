{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quickdraw_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8YqIzdR0HDZ",
        "colab_type": "code",
        "outputId": "127ba6da-81bd-4277-ad44-97814457c77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import struct\n",
        "from struct import unpack\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device = {}'.format(device))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device = cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8hPIpN33EAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper from: https://github.com/googlecreativelab/quickdraw-dataset/blob/master/examples/binary_file_parser.py\n",
        "def unpack_drawing(file_handle, max_padding):\n",
        "    # Skip key_id: 8, countrycode: 2, recognized: 1, timestamp: 4 = 15\n",
        "    file_handle.read(15)\n",
        "    n_strokes, = unpack('H', file_handle.read(2))\n",
        "    idx = 0\n",
        "\n",
        "    N = 0\n",
        "    strokes = []\n",
        "    for i in range(n_strokes):\n",
        "      n_points, = unpack('H', file_handle.read(2))\n",
        "      N += n_points\n",
        "      fmt = str(n_points) + 'B'\n",
        "      x = unpack(fmt, file_handle.read(n_points))\n",
        "      y = unpack(fmt, file_handle.read(n_points))\n",
        "      strokes.append((x, y))\n",
        "\n",
        "    image = np.zeros((N, 3), dtype=np.float32)\n",
        "\n",
        "\n",
        "    \n",
        "    # Return a tensor of size number of stroke x 3 like here: https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/recurrent_quickdraw.md#optional-converting-the-data\n",
        "    for i, (x, y) in enumerate(strokes):\n",
        "        n_points = len(x)\n",
        "        image[idx:idx+n_points, 0] = np.asarray(x)\n",
        "        image[idx:idx+n_points, 1] = np.asarray(y)\n",
        "        idx += n_points\n",
        "        # Mark stroke end with a 1\n",
        "        image[idx -1, 2] = 1\n",
        "\n",
        "\n",
        "    # Preprocessing.\n",
        "    # 1. Size normalization.\n",
        "    lower = np.min(image[:, 0:2], axis=0)\n",
        "    upper = np.max(image[:, 0:2], axis=0)\n",
        "    scale = upper - lower\n",
        "    scale[scale == 0] = 1\n",
        "    image[:, 0:2] = (image[:, 0:2] - lower) / scale\n",
        "    # 2. Compute deltas.\n",
        "    image[1:, 0:2] -= image[0:-1, 0:2]\n",
        "    image = image[1:, :]\n",
        "\n",
        "    return torch.from_numpy(image[:max_padding])\n",
        "\n",
        "\n",
        "def unpack_drawings(filename, max_padding):\n",
        "    with open(filename, 'rb') as f:\n",
        "        while True:\n",
        "            try:\n",
        "                yield unpack_drawing(f, max_padding)\n",
        "            except struct.error:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3VBKmsH3OD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "urllib.request.urlretrieve('https://raw.githubusercontent.com/cs-deep-quickdraw/notebooks/master/100_classes.txt', '100_classes.txt')\n",
        "\n",
        "# Create data dir\n",
        "Path(\"./data\").mkdir(exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5eq4npv3fsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"100_classes.txt\",\"r\")\n",
        "# And for reading use\n",
        "classes = [cls.strip() for cls in f.readlines()]\n",
        "f.close()\n",
        "\n",
        "def download(classes):\n",
        "  base = 'https://storage.googleapis.com/quickdraw_dataset/full/binary/'\n",
        "  for i, c in enumerate(classes):\n",
        "    cls_url = c.replace('_', '%20')\n",
        "    path = base+cls_url+'.bin'\n",
        "    print((1+i)/len(classes), c, path)\n",
        "    urllib.request.urlretrieve(path, 'data/'+c+'.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yms70h23jSQ",
        "colab_type": "code",
        "outputId": "b8a8ae78-d536-498b-a008-b49e5a2f0c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "download(classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01 drums https://storage.googleapis.com/quickdraw_dataset/full/binary/drums.bin\n",
            "0.02 sun https://storage.googleapis.com/quickdraw_dataset/full/binary/sun.bin\n",
            "0.03 laptop https://storage.googleapis.com/quickdraw_dataset/full/binary/laptop.bin\n",
            "0.04 anvil https://storage.googleapis.com/quickdraw_dataset/full/binary/anvil.bin\n",
            "0.05 baseball_bat https://storage.googleapis.com/quickdraw_dataset/full/binary/baseball%20bat.bin\n",
            "0.06 ladder https://storage.googleapis.com/quickdraw_dataset/full/binary/ladder.bin\n",
            "0.07 eyeglasses https://storage.googleapis.com/quickdraw_dataset/full/binary/eyeglasses.bin\n",
            "0.08 grapes https://storage.googleapis.com/quickdraw_dataset/full/binary/grapes.bin\n",
            "0.09 book https://storage.googleapis.com/quickdraw_dataset/full/binary/book.bin\n",
            "0.1 dumbbell https://storage.googleapis.com/quickdraw_dataset/full/binary/dumbbell.bin\n",
            "0.11 traffic_light https://storage.googleapis.com/quickdraw_dataset/full/binary/traffic%20light.bin\n",
            "0.12 wristwatch https://storage.googleapis.com/quickdraw_dataset/full/binary/wristwatch.bin\n",
            "0.13 wheel https://storage.googleapis.com/quickdraw_dataset/full/binary/wheel.bin\n",
            "0.14 shovel https://storage.googleapis.com/quickdraw_dataset/full/binary/shovel.bin\n",
            "0.15 bread https://storage.googleapis.com/quickdraw_dataset/full/binary/bread.bin\n",
            "0.16 table https://storage.googleapis.com/quickdraw_dataset/full/binary/table.bin\n",
            "0.17 tennis_racquet https://storage.googleapis.com/quickdraw_dataset/full/binary/tennis%20racquet.bin\n",
            "0.18 cloud https://storage.googleapis.com/quickdraw_dataset/full/binary/cloud.bin\n",
            "0.19 chair https://storage.googleapis.com/quickdraw_dataset/full/binary/chair.bin\n",
            "0.2 headphones https://storage.googleapis.com/quickdraw_dataset/full/binary/headphones.bin\n",
            "0.21 face https://storage.googleapis.com/quickdraw_dataset/full/binary/face.bin\n",
            "0.22 eye https://storage.googleapis.com/quickdraw_dataset/full/binary/eye.bin\n",
            "0.23 airplane https://storage.googleapis.com/quickdraw_dataset/full/binary/airplane.bin\n",
            "0.24 snake https://storage.googleapis.com/quickdraw_dataset/full/binary/snake.bin\n",
            "0.25 lollipop https://storage.googleapis.com/quickdraw_dataset/full/binary/lollipop.bin\n",
            "0.26 power_outlet https://storage.googleapis.com/quickdraw_dataset/full/binary/power%20outlet.bin\n",
            "0.27 pants https://storage.googleapis.com/quickdraw_dataset/full/binary/pants.bin\n",
            "0.28 mushroom https://storage.googleapis.com/quickdraw_dataset/full/binary/mushroom.bin\n",
            "0.29 star https://storage.googleapis.com/quickdraw_dataset/full/binary/star.bin\n",
            "0.3 sword https://storage.googleapis.com/quickdraw_dataset/full/binary/sword.bin\n",
            "0.31 clock https://storage.googleapis.com/quickdraw_dataset/full/binary/clock.bin\n",
            "0.32 hot_dog https://storage.googleapis.com/quickdraw_dataset/full/binary/hot%20dog.bin\n",
            "0.33 syringe https://storage.googleapis.com/quickdraw_dataset/full/binary/syringe.bin\n",
            "0.34 stop_sign https://storage.googleapis.com/quickdraw_dataset/full/binary/stop%20sign.bin\n",
            "0.35 mountain https://storage.googleapis.com/quickdraw_dataset/full/binary/mountain.bin\n",
            "0.36 smiley_face https://storage.googleapis.com/quickdraw_dataset/full/binary/smiley%20face.bin\n",
            "0.37 apple https://storage.googleapis.com/quickdraw_dataset/full/binary/apple.bin\n",
            "0.38 bed https://storage.googleapis.com/quickdraw_dataset/full/binary/bed.bin\n",
            "0.39 shorts https://storage.googleapis.com/quickdraw_dataset/full/binary/shorts.bin\n",
            "0.4 broom https://storage.googleapis.com/quickdraw_dataset/full/binary/broom.bin\n",
            "0.41 diving_board https://storage.googleapis.com/quickdraw_dataset/full/binary/diving%20board.bin\n",
            "0.42 flower https://storage.googleapis.com/quickdraw_dataset/full/binary/flower.bin\n",
            "0.43 spider https://storage.googleapis.com/quickdraw_dataset/full/binary/spider.bin\n",
            "0.44 cell_phone https://storage.googleapis.com/quickdraw_dataset/full/binary/cell%20phone.bin\n",
            "0.45 car https://storage.googleapis.com/quickdraw_dataset/full/binary/car.bin\n",
            "0.46 camera https://storage.googleapis.com/quickdraw_dataset/full/binary/camera.bin\n",
            "0.47 tree https://storage.googleapis.com/quickdraw_dataset/full/binary/tree.bin\n",
            "0.48 square https://storage.googleapis.com/quickdraw_dataset/full/binary/square.bin\n",
            "0.49 moon https://storage.googleapis.com/quickdraw_dataset/full/binary/moon.bin\n",
            "0.5 radio https://storage.googleapis.com/quickdraw_dataset/full/binary/radio.bin\n",
            "0.51 hat https://storage.googleapis.com/quickdraw_dataset/full/binary/hat.bin\n",
            "0.52 pizza https://storage.googleapis.com/quickdraw_dataset/full/binary/pizza.bin\n",
            "0.53 axe https://storage.googleapis.com/quickdraw_dataset/full/binary/axe.bin\n",
            "0.54 door https://storage.googleapis.com/quickdraw_dataset/full/binary/door.bin\n",
            "0.55 tent https://storage.googleapis.com/quickdraw_dataset/full/binary/tent.bin\n",
            "0.56 umbrella https://storage.googleapis.com/quickdraw_dataset/full/binary/umbrella.bin\n",
            "0.57 line https://storage.googleapis.com/quickdraw_dataset/full/binary/line.bin\n",
            "0.58 cup https://storage.googleapis.com/quickdraw_dataset/full/binary/cup.bin\n",
            "0.59 fan https://storage.googleapis.com/quickdraw_dataset/full/binary/fan.bin\n",
            "0.6 triangle https://storage.googleapis.com/quickdraw_dataset/full/binary/triangle.bin\n",
            "0.61 basketball https://storage.googleapis.com/quickdraw_dataset/full/binary/basketball.bin\n",
            "0.62 pillow https://storage.googleapis.com/quickdraw_dataset/full/binary/pillow.bin\n",
            "0.63 scissors https://storage.googleapis.com/quickdraw_dataset/full/binary/scissors.bin\n",
            "0.64 t-shirt https://storage.googleapis.com/quickdraw_dataset/full/binary/t-shirt.bin\n",
            "0.65 tooth https://storage.googleapis.com/quickdraw_dataset/full/binary/tooth.bin\n",
            "0.66 alarm_clock https://storage.googleapis.com/quickdraw_dataset/full/binary/alarm%20clock.bin\n",
            "0.67 paper_clip https://storage.googleapis.com/quickdraw_dataset/full/binary/paper%20clip.bin\n",
            "0.68 spoon https://storage.googleapis.com/quickdraw_dataset/full/binary/spoon.bin\n",
            "0.69 microphone https://storage.googleapis.com/quickdraw_dataset/full/binary/microphone.bin\n",
            "0.7 candle https://storage.googleapis.com/quickdraw_dataset/full/binary/candle.bin\n",
            "0.71 pencil https://storage.googleapis.com/quickdraw_dataset/full/binary/pencil.bin\n",
            "0.72 envelope https://storage.googleapis.com/quickdraw_dataset/full/binary/envelope.bin\n",
            "0.73 saw https://storage.googleapis.com/quickdraw_dataset/full/binary/saw.bin\n",
            "0.74 frying_pan https://storage.googleapis.com/quickdraw_dataset/full/binary/frying%20pan.bin\n",
            "0.75 screwdriver https://storage.googleapis.com/quickdraw_dataset/full/binary/screwdriver.bin\n",
            "0.76 helmet https://storage.googleapis.com/quickdraw_dataset/full/binary/helmet.bin\n",
            "0.77 bridge https://storage.googleapis.com/quickdraw_dataset/full/binary/bridge.bin\n",
            "0.78 light_bulb https://storage.googleapis.com/quickdraw_dataset/full/binary/light%20bulb.bin\n",
            "0.79 ceiling_fan https://storage.googleapis.com/quickdraw_dataset/full/binary/ceiling%20fan.bin\n",
            "0.8 key https://storage.googleapis.com/quickdraw_dataset/full/binary/key.bin\n",
            "0.81 donut https://storage.googleapis.com/quickdraw_dataset/full/binary/donut.bin\n",
            "0.82 bird https://storage.googleapis.com/quickdraw_dataset/full/binary/bird.bin\n",
            "0.83 circle https://storage.googleapis.com/quickdraw_dataset/full/binary/circle.bin\n",
            "0.84 beard https://storage.googleapis.com/quickdraw_dataset/full/binary/beard.bin\n",
            "0.85 coffee_cup https://storage.googleapis.com/quickdraw_dataset/full/binary/coffee%20cup.bin\n",
            "0.86 butterfly https://storage.googleapis.com/quickdraw_dataset/full/binary/butterfly.bin\n",
            "0.87 bench https://storage.googleapis.com/quickdraw_dataset/full/binary/bench.bin\n",
            "0.88 rifle https://storage.googleapis.com/quickdraw_dataset/full/binary/rifle.bin\n",
            "0.89 cat https://storage.googleapis.com/quickdraw_dataset/full/binary/cat.bin\n",
            "0.9 sock https://storage.googleapis.com/quickdraw_dataset/full/binary/sock.bin\n",
            "0.91 ice_cream https://storage.googleapis.com/quickdraw_dataset/full/binary/ice%20cream.bin\n",
            "0.92 moustache https://storage.googleapis.com/quickdraw_dataset/full/binary/moustache.bin\n",
            "0.93 suitcase https://storage.googleapis.com/quickdraw_dataset/full/binary/suitcase.bin\n",
            "0.94 hammer https://storage.googleapis.com/quickdraw_dataset/full/binary/hammer.bin\n",
            "0.95 rainbow https://storage.googleapis.com/quickdraw_dataset/full/binary/rainbow.bin\n",
            "0.96 knife https://storage.googleapis.com/quickdraw_dataset/full/binary/knife.bin\n",
            "0.97 cookie https://storage.googleapis.com/quickdraw_dataset/full/binary/cookie.bin\n",
            "0.98 baseball https://storage.googleapis.com/quickdraw_dataset/full/binary/baseball.bin\n",
            "0.99 lightning https://storage.googleapis.com/quickdraw_dataset/full/binary/lightning.bin\n",
            "1.0 bicycle https://storage.googleapis.com/quickdraw_dataset/full/binary/bicycle.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB0d6bjaC8mD",
        "colab_type": "code",
        "outputId": "741672bb-f10a-48f1-a1aa-2084acb96d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "print(len(os.listdir('data')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBUTUh0MFI11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StrokeClassifier(nn.Module):\n",
        "  def __init__(self, cv1, cv2, hidden_dim, n_layers, n_classes, bidirectional):\n",
        "    super(StrokeClassifier, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    input_size = 3\n",
        "\n",
        "    self.bn = nn.BatchNorm1d(input_size)\n",
        "\n",
        "    if cv1 is not None:\n",
        "      self.conv1 = nn.Conv1d(input_size, cv1[0], cv1[1])\n",
        "      input_size = cv1[0]\n",
        "\n",
        "      if cv2 is not None:\n",
        "        self.conv2 = nn.Conv1d(input_size, cv2[0], cv2[1])\n",
        "        input_size = cv2[0]\n",
        "      else:\n",
        "        self.conv2 = None\n",
        "    else:\n",
        "      self.conv1 = None\n",
        "      self.conv2 = None\n",
        "    \n",
        "    # The LSTM takes 3 things as input (x, y, isLastPoint) and outputs hidden states with dimensionality hidden_dim\n",
        "    self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "    # The linear layer maps the LSTM output to a linear space\n",
        "    self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, n_classes)\n",
        "\n",
        "  def forward(self, strokes):\n",
        "    # BN and Convolution expect NCWH\n",
        "    strokes = self.bn(strokes)\n",
        "    if self.conv1 is not None:\n",
        "      strokes = self.conv1(strokes)\n",
        "\n",
        "      if self.conv2 is not None:\n",
        "        strokes = self.conv2(strokes)\n",
        "\n",
        "    # LSTM expect NHWC\n",
        "    strokes = torch.transpose(strokes, 1, 2)\n",
        "    out, _ = self.lstm(strokes)\n",
        "\n",
        "    # Keep last layer of the NN\n",
        "    out = out[:,-1,:]\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-bafz08v3BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DrawDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        assert len(self.X) == len(self.Y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        return (self.X[idx], self.Y[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQge5B5H-pKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Config:\n",
        "batch_size = 256\n",
        "learning_rate = 0.001\n",
        "\n",
        "hidden_size = 256\n",
        "n_layers = 2\n",
        "train_classes = classes[:]\n",
        "\n",
        "# Use None instead of (n_filters, filter_size) to disable convolution\n",
        "# Note that conv1 = None forces conv2 = None automatically\n",
        "conv1 = (128, 3)\n",
        "conv2 = None\n",
        "bidirectional = True\n",
        "\n",
        "N_train = 20000\n",
        "N_val = N_train // 5\n",
        "N_test = N_val\n",
        "N_test_reserved = 20000\n",
        "max_padding = 100\n",
        "n_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJPBw8l_WLvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import islice\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def extract_dataset(samples_train, samples_val, samples_test, test_reserved, classes, max_padding=100):\n",
        "  X_train = []\n",
        "  X_val = []\n",
        "  X_test = []\n",
        "  y_train = []\n",
        "  y_val = []\n",
        "  y_test = []\n",
        "\n",
        "  for c, cls in enumerate(classes):\n",
        "    drawings = unpack_drawings('data/' + cls + '.bin', max_padding)\n",
        "\n",
        "    # TODO: itertools\n",
        "    for _ in range(max(0, test_reserved - samples_train)):\n",
        "      next(drawings)\n",
        "\n",
        "    for _ in range(samples_test):\n",
        "      X_test.append(next(drawings))\n",
        "      y_test.append(c)\n",
        "\n",
        "    # TODO: better way of doing this\n",
        "    for _ in range(samples_train):\n",
        "      X_train.append(next(drawings))\n",
        "      y_train.append(c)\n",
        "\n",
        "    for _ in range(samples_val):\n",
        "      X_val.append(next(drawings))\n",
        "      y_val.append(c)\n",
        "\n",
        "\n",
        "  \n",
        "    \n",
        "    print(f\"\\rdone extracting class: {cls}: {1 + c} / {len(classes)}\", end='')\n",
        "\n",
        "    drawings.close()\n",
        "    \n",
        "\n",
        "  def norm(X):\n",
        "    return torch.FloatTensor(torch.transpose(pad_sequence(X, batch_first=True), 1, 2))\n",
        "\n",
        "  # XXX: instead of padding like that we could have a moving window:\n",
        "  # Example if we want 100 sequences and we have an image with 200 we can use the windows:\n",
        "  # 0-100, 10-110, ... 100-200 for instance, this would add data\n",
        "  X_train = norm(X_train)\n",
        "  X_val = norm(X_val)\n",
        "  X_test = norm(X_test)\n",
        "  #print(\"training shape\", X_train.shape)\n",
        "  print(\"validation shape\", X_val.shape)\n",
        "  print(\"testing shape\", X_test.shape)\n",
        "  print(\"classes\", len(classes))\n",
        "\n",
        "  return (\n",
        "      DrawDataset(X_train, torch.LongTensor(y_train)), \n",
        "      DrawDataset(X_val, torch.LongTensor(y_val)),\n",
        "      DrawDataset(X_test, torch.LongTensor(y_test)),\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jp5Zo6WcN2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, loader):\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for i, (img, label) in enumerate(loader):\n",
        "      img = img.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      out = model(img)\n",
        "\n",
        "      _, pred = torch.max(out.data, 1)\n",
        "\n",
        "      total += label.size(0)\n",
        "      correct += (pred == label).sum().item()\n",
        "\n",
        "    return 100. * correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eysyPVD42XAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training(losses, accs, n_epochs):\n",
        "  fig, ax1 = plt.subplots()\n",
        "\n",
        "  color = 'tab:red'\n",
        "  ax1.set_xlabel('epoch')\n",
        "  ax1.set_ylabel('training loss', color=color)\n",
        "  ax1.plot(losses, color=color)\n",
        "  ax1.tick_params(axis='y', labelcolor=color)\n",
        "  ax1.set_ylim([0, 5])\n",
        "\n",
        "  ax1.set_xlim([0, n_epochs])\n",
        "  ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "  color = 'tab:blue'\n",
        "  ax2.set_ylabel('validation accuracy', color=color)  # we already handled the x-label with ax1\n",
        "  ax2.plot(accs, color=color)\n",
        "  ax2.tick_params(axis='y', labelcolor=color)\n",
        "  ax2.set_ylim([0, 100])\n",
        "\n",
        "  fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Aua8_l2igl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "def train_model(model, opt, loss_fn, loader, v_loader, n_epochs):\n",
        "\n",
        "  best_acc, best_model = 0, None\n",
        "  losses, accs = [], []\n",
        "  for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    epoch_losses = []\n",
        "    for i, (img, lab) in enumerate(loader):\n",
        "      print(f\"\\rbatch: {i}, current loss: {np.mean(epoch_losses) if epoch_losses else 'NaN'}\", end='')\n",
        "      img = img.to(device)\n",
        "      lab = lab.to(device)\n",
        "\n",
        "      out = model(img)\n",
        "\n",
        "      loss = loss_fn(out, lab)\n",
        "\n",
        "      opt.zero_grad()\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      epoch_losses.append(loss.item())\n",
        "\n",
        "    print(\"\\rEvaluating model on validation dataset...\", end='')\n",
        "    val_acc = evaluate_model(model, v_loader)\n",
        "    mean_loss = np.mean(epoch_losses)\n",
        "\n",
        "    losses.append(mean_loss)\n",
        "    accs.append(val_acc)\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "      best_acc = val_acc\n",
        "      best_model = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model, f\"lstm_epoch_{epoch}_acc_{val_acc}.model\")\n",
        "\n",
        "    print(f\"\\rEpoch: {epoch+1}/{n_epochs}, loss: {mean_loss}, validation accuracy: {val_acc}% took: {time.time() - start} seconds\")\n",
        "\n",
        "  print(f\"Training ended after {n_epochs} ! Best validation accuracy: {best_acc}%\")\n",
        "  try:\n",
        "    plot_training(losses, accs, n_epochs)\n",
        "  except:\n",
        "    print(\"error occurred when plotting losses and accuracy training data\")\n",
        "  return best_model, losses, accs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaXyrAEfvap6",
        "colab_type": "code",
        "outputId": "c90fde70-dbea-4cae-9573-fc40dc9bb1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# TODO: really take the last 2k images for testing\n",
        "train_dataset, val_dataset, test_dataset = extract_dataset(N_train, N_val, N_test, N_test_reserved, train_classes, max_padding=max_padding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done extracting class: drums: 1 / 100\n",
            "done extracting class: traffic_light: 11 / 100\n",
            "done extracting class: face: 21 / 100\n",
            "done extracting class: clock: 31 / 100\n",
            "done extracting class: diving_board: 41 / 100\n",
            "done extracting class: hat: 51 / 100\n",
            "done extracting class: basketball: 61 / 100\n",
            "done extracting class: pencil: 71 / 100\n",
            "done extracting class: donut: 81 / 100\n",
            "done extracting class: ice_cream: 91 / 100\n",
            "validation shape torch.Size([400000, 3, 100])\n",
            "testing shape torch.Size([400000, 3, 100])\n",
            "classes 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEGoqxR1y-6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaS_SobxWokC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = StrokeClassifier(conv1, conv2, hidden_size, n_layers, len(train_classes), bidirectional).to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F2SK1L5YWOQ",
        "colab_type": "code",
        "outputId": "ee62e914-2ee7-4098-f8c6-32a6efbf4a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "source": [
        "best_model, losses, accs = train_model(model, optimizer, loss_function, train_loader, val_loader, n_epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10, loss: 1.4458234518457433, validation accuracy: 70.179% took: 630.9494168758392 seconds\n",
            "Epoch: 2/10, loss: 0.6833124341800547, validation accuracy: 77.27725% took: 630.3967542648315 seconds\n",
            "Epoch: 3/10, loss: 0.5669822885195821, validation accuracy: 79.38775% took: 629.7384443283081 seconds\n",
            "Epoch: 4/10, loss: 0.5058896666785812, validation accuracy: 81.322% took: 630.7445013523102 seconds\n",
            "Epoch: 5/10, loss: 0.4642114854283878, validation accuracy: 81.59575% took: 630.1018018722534 seconds\n",
            "Epoch: 6/10, loss: 0.43246729940111167, validation accuracy: 82.56975% took: 630.7682511806488 seconds\n",
            "Epoch: 7/10, loss: 0.4056210992935295, validation accuracy: 82.552% took: 629.5818972587585 seconds\n",
            "Epoch: 8/10, loss: 0.38292044424818433, validation accuracy: 82.611% took: 630.5734429359436 seconds\n",
            "Epoch: 9/10, loss: 0.3625405829137104, validation accuracy: 83.031% took: 631.2119917869568 seconds\n",
            "Epoch: 10/10, loss: 0.34431914681740156, validation accuracy: 82.712% took: 630.0399312973022 seconds\n",
            "Training ended after 10 ! Best validation accuracy: 83.031%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bynKPlEifCuI",
        "colab_type": "code",
        "outputId": "ed20a7ce-e463-4126-e739-1eca07ac68ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"Test accuracy: {evaluate_model(model, test_loader)}%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 79.715%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaCHS8rMi7DK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model_path = f'lstm_quickdraw.model.{time.time()}'\n",
        "torch.save(best_model, model_path)\n",
        "\n",
        "print(f\"Model saved at: {model_path}\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}