{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quickdraw_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8YqIzdR0HDZ",
        "colab_type": "code",
        "outputId": "127ba6da-81bd-4277-ad44-97814457c77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import struct\n",
        "from struct import unpack\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device = {}'.format(device))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device = cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8hPIpN33EAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper from: https://github.com/googlecreativelab/quickdraw-dataset/blob/master/examples/binary_file_parser.py\n",
        "def unpack_drawing(file_handle, max_padding):\n",
        "    # Skip key_id: 8, countrycode: 2, recognized: 1, timestamp: 4 = 15\n",
        "    file_handle.read(15)\n",
        "    n_strokes, = unpack('H', file_handle.read(2))\n",
        "    idx = 0\n",
        "\n",
        "    N = 0\n",
        "    strokes = []\n",
        "    for i in range(n_strokes):\n",
        "      n_points, = unpack('H', file_handle.read(2))\n",
        "      N += n_points\n",
        "      fmt = str(n_points) + 'B'\n",
        "      x = unpack(fmt, file_handle.read(n_points))\n",
        "      y = unpack(fmt, file_handle.read(n_points))\n",
        "      strokes.append((x, y))\n",
        "\n",
        "    image = np.zeros((N, 3), dtype=np.float32)\n",
        "\n",
        "\n",
        "    \n",
        "    # Return a tensor of size number of stroke x 3 like here: https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/recurrent_quickdraw.md#optional-converting-the-data\n",
        "    for i, (x, y) in enumerate(strokes):\n",
        "        n_points = len(x)\n",
        "        image[idx:idx+n_points, 0] = np.asarray(x)\n",
        "        image[idx:idx+n_points, 1] = np.asarray(y)\n",
        "        idx += n_points\n",
        "        # Mark stroke end with a 1\n",
        "        image[idx -1, 2] = 1\n",
        "\n",
        "\n",
        "    # Preprocessing.\n",
        "    # 1. Size normalization.\n",
        "    lower = np.min(image[:, 0:2], axis=0)\n",
        "    upper = np.max(image[:, 0:2], axis=0)\n",
        "    scale = upper - lower\n",
        "    scale[scale == 0] = 1\n",
        "    image[:, 0:2] = (image[:, 0:2] - lower) / scale\n",
        "    # 2. Compute deltas.\n",
        "    image[1:, 0:2] -= image[0:-1, 0:2]\n",
        "    image = image[1:, :]\n",
        "\n",
        "    return torch.from_numpy(image[:max_padding])\n",
        "\n",
        "\n",
        "def unpack_drawings(filename, max_padding):\n",
        "    with open(filename, 'rb') as f:\n",
        "        while True:\n",
        "            try:\n",
        "                yield unpack_drawing(f, max_padding)\n",
        "            except struct.error:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3VBKmsH3OD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "urllib.request.urlretrieve('https://raw.githubusercontent.com/cs-deep-quickdraw/notebooks/master/100_classes.txt', '100_classes.txt')\n",
        "\n",
        "# Create data dir\n",
        "Path(\"./data\").mkdir(exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5eq4npv3fsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"100_classes.txt\",\"r\")\n",
        "# And for reading use\n",
        "classes = [cls.strip() for cls in f.readlines()]\n",
        "f.close()\n",
        "\n",
        "def download(classes):\n",
        "  base = 'https://storage.googleapis.com/quickdraw_dataset/full/binary/'\n",
        "  for i, c in enumerate(classes):\n",
        "    cls_url = c.replace('_', '%20')\n",
        "    path = base+cls_url+'.bin'\n",
        "    print((1+i)/len(classes), c, path)\n",
        "    urllib.request.urlretrieve(path, 'data/'+c+'.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yms70h23jSQ",
        "colab_type": "code",
        "outputId": "b8a8ae78-d536-498b-a008-b49e5a2f0c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "download(classes)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01 drums https://storage.googleapis.com/quickdraw_dataset/full/binary/drums.bin\n",
            "0.02 sun https://storage.googleapis.com/quickdraw_dataset/full/binary/sun.bin\n",
            "0.03 laptop https://storage.googleapis.com/quickdraw_dataset/full/binary/laptop.bin\n",
            "0.04 anvil https://storage.googleapis.com/quickdraw_dataset/full/binary/anvil.bin\n",
            "0.05 baseball_bat https://storage.googleapis.com/quickdraw_dataset/full/binary/baseball%20bat.bin\n",
            "0.06 ladder https://storage.googleapis.com/quickdraw_dataset/full/binary/ladder.bin\n",
            "0.07 eyeglasses https://storage.googleapis.com/quickdraw_dataset/full/binary/eyeglasses.bin\n",
            "0.08 grapes https://storage.googleapis.com/quickdraw_dataset/full/binary/grapes.bin\n",
            "0.09 book https://storage.googleapis.com/quickdraw_dataset/full/binary/book.bin\n",
            "0.1 dumbbell https://storage.googleapis.com/quickdraw_dataset/full/binary/dumbbell.bin\n",
            "0.11 traffic_light https://storage.googleapis.com/quickdraw_dataset/full/binary/traffic%20light.bin\n",
            "0.12 wristwatch https://storage.googleapis.com/quickdraw_dataset/full/binary/wristwatch.bin\n",
            "0.13 wheel https://storage.googleapis.com/quickdraw_dataset/full/binary/wheel.bin\n",
            "0.14 shovel https://storage.googleapis.com/quickdraw_dataset/full/binary/shovel.bin\n",
            "0.15 bread https://storage.googleapis.com/quickdraw_dataset/full/binary/bread.bin\n",
            "0.16 table https://storage.googleapis.com/quickdraw_dataset/full/binary/table.bin\n",
            "0.17 tennis_racquet https://storage.googleapis.com/quickdraw_dataset/full/binary/tennis%20racquet.bin\n",
            "0.18 cloud https://storage.googleapis.com/quickdraw_dataset/full/binary/cloud.bin\n",
            "0.19 chair https://storage.googleapis.com/quickdraw_dataset/full/binary/chair.bin\n",
            "0.2 headphones https://storage.googleapis.com/quickdraw_dataset/full/binary/headphones.bin\n",
            "0.21 face https://storage.googleapis.com/quickdraw_dataset/full/binary/face.bin\n",
            "0.22 eye https://storage.googleapis.com/quickdraw_dataset/full/binary/eye.bin\n",
            "0.23 airplane https://storage.googleapis.com/quickdraw_dataset/full/binary/airplane.bin\n",
            "0.24 snake https://storage.googleapis.com/quickdraw_dataset/full/binary/snake.bin\n",
            "0.25 lollipop https://storage.googleapis.com/quickdraw_dataset/full/binary/lollipop.bin\n",
            "0.26 power_outlet https://storage.googleapis.com/quickdraw_dataset/full/binary/power%20outlet.bin\n",
            "0.27 pants https://storage.googleapis.com/quickdraw_dataset/full/binary/pants.bin\n",
            "0.28 mushroom https://storage.googleapis.com/quickdraw_dataset/full/binary/mushroom.bin\n",
            "0.29 star https://storage.googleapis.com/quickdraw_dataset/full/binary/star.bin\n",
            "0.3 sword https://storage.googleapis.com/quickdraw_dataset/full/binary/sword.bin\n",
            "0.31 clock https://storage.googleapis.com/quickdraw_dataset/full/binary/clock.bin\n",
            "0.32 hot_dog https://storage.googleapis.com/quickdraw_dataset/full/binary/hot%20dog.bin\n",
            "0.33 syringe https://storage.googleapis.com/quickdraw_dataset/full/binary/syringe.bin\n",
            "0.34 stop_sign https://storage.googleapis.com/quickdraw_dataset/full/binary/stop%20sign.bin\n",
            "0.35 mountain https://storage.googleapis.com/quickdraw_dataset/full/binary/mountain.bin\n",
            "0.36 smiley_face https://storage.googleapis.com/quickdraw_dataset/full/binary/smiley%20face.bin\n",
            "0.37 apple https://storage.googleapis.com/quickdraw_dataset/full/binary/apple.bin\n",
            "0.38 bed https://storage.googleapis.com/quickdraw_dataset/full/binary/bed.bin\n",
            "0.39 shorts https://storage.googleapis.com/quickdraw_dataset/full/binary/shorts.bin\n",
            "0.4 broom https://storage.googleapis.com/quickdraw_dataset/full/binary/broom.bin\n",
            "0.41 diving_board https://storage.googleapis.com/quickdraw_dataset/full/binary/diving%20board.bin\n",
            "0.42 flower https://storage.googleapis.com/quickdraw_dataset/full/binary/flower.bin\n",
            "0.43 spider https://storage.googleapis.com/quickdraw_dataset/full/binary/spider.bin\n",
            "0.44 cell_phone https://storage.googleapis.com/quickdraw_dataset/full/binary/cell%20phone.bin\n",
            "0.45 car https://storage.googleapis.com/quickdraw_dataset/full/binary/car.bin\n",
            "0.46 camera https://storage.googleapis.com/quickdraw_dataset/full/binary/camera.bin\n",
            "0.47 tree https://storage.googleapis.com/quickdraw_dataset/full/binary/tree.bin\n",
            "0.48 square https://storage.googleapis.com/quickdraw_dataset/full/binary/square.bin\n",
            "0.49 moon https://storage.googleapis.com/quickdraw_dataset/full/binary/moon.bin\n",
            "0.5 radio https://storage.googleapis.com/quickdraw_dataset/full/binary/radio.bin\n",
            "0.51 hat https://storage.googleapis.com/quickdraw_dataset/full/binary/hat.bin\n",
            "0.52 pizza https://storage.googleapis.com/quickdraw_dataset/full/binary/pizza.bin\n",
            "0.53 axe https://storage.googleapis.com/quickdraw_dataset/full/binary/axe.bin\n",
            "0.54 door https://storage.googleapis.com/quickdraw_dataset/full/binary/door.bin\n",
            "0.55 tent https://storage.googleapis.com/quickdraw_dataset/full/binary/tent.bin\n",
            "0.56 umbrella https://storage.googleapis.com/quickdraw_dataset/full/binary/umbrella.bin\n",
            "0.57 line https://storage.googleapis.com/quickdraw_dataset/full/binary/line.bin\n",
            "0.58 cup https://storage.googleapis.com/quickdraw_dataset/full/binary/cup.bin\n",
            "0.59 fan https://storage.googleapis.com/quickdraw_dataset/full/binary/fan.bin\n",
            "0.6 triangle https://storage.googleapis.com/quickdraw_dataset/full/binary/triangle.bin\n",
            "0.61 basketball https://storage.googleapis.com/quickdraw_dataset/full/binary/basketball.bin\n",
            "0.62 pillow https://storage.googleapis.com/quickdraw_dataset/full/binary/pillow.bin\n",
            "0.63 scissors https://storage.googleapis.com/quickdraw_dataset/full/binary/scissors.bin\n",
            "0.64 t-shirt https://storage.googleapis.com/quickdraw_dataset/full/binary/t-shirt.bin\n",
            "0.65 tooth https://storage.googleapis.com/quickdraw_dataset/full/binary/tooth.bin\n",
            "0.66 alarm_clock https://storage.googleapis.com/quickdraw_dataset/full/binary/alarm%20clock.bin\n",
            "0.67 paper_clip https://storage.googleapis.com/quickdraw_dataset/full/binary/paper%20clip.bin\n",
            "0.68 spoon https://storage.googleapis.com/quickdraw_dataset/full/binary/spoon.bin\n",
            "0.69 microphone https://storage.googleapis.com/quickdraw_dataset/full/binary/microphone.bin\n",
            "0.7 candle https://storage.googleapis.com/quickdraw_dataset/full/binary/candle.bin\n",
            "0.71 pencil https://storage.googleapis.com/quickdraw_dataset/full/binary/pencil.bin\n",
            "0.72 envelope https://storage.googleapis.com/quickdraw_dataset/full/binary/envelope.bin\n",
            "0.73 saw https://storage.googleapis.com/quickdraw_dataset/full/binary/saw.bin\n",
            "0.74 frying_pan https://storage.googleapis.com/quickdraw_dataset/full/binary/frying%20pan.bin\n",
            "0.75 screwdriver https://storage.googleapis.com/quickdraw_dataset/full/binary/screwdriver.bin\n",
            "0.76 helmet https://storage.googleapis.com/quickdraw_dataset/full/binary/helmet.bin\n",
            "0.77 bridge https://storage.googleapis.com/quickdraw_dataset/full/binary/bridge.bin\n",
            "0.78 light_bulb https://storage.googleapis.com/quickdraw_dataset/full/binary/light%20bulb.bin\n",
            "0.79 ceiling_fan https://storage.googleapis.com/quickdraw_dataset/full/binary/ceiling%20fan.bin\n",
            "0.8 key https://storage.googleapis.com/quickdraw_dataset/full/binary/key.bin\n",
            "0.81 donut https://storage.googleapis.com/quickdraw_dataset/full/binary/donut.bin\n",
            "0.82 bird https://storage.googleapis.com/quickdraw_dataset/full/binary/bird.bin\n",
            "0.83 circle https://storage.googleapis.com/quickdraw_dataset/full/binary/circle.bin\n",
            "0.84 beard https://storage.googleapis.com/quickdraw_dataset/full/binary/beard.bin\n",
            "0.85 coffee_cup https://storage.googleapis.com/quickdraw_dataset/full/binary/coffee%20cup.bin\n",
            "0.86 butterfly https://storage.googleapis.com/quickdraw_dataset/full/binary/butterfly.bin\n",
            "0.87 bench https://storage.googleapis.com/quickdraw_dataset/full/binary/bench.bin\n",
            "0.88 rifle https://storage.googleapis.com/quickdraw_dataset/full/binary/rifle.bin\n",
            "0.89 cat https://storage.googleapis.com/quickdraw_dataset/full/binary/cat.bin\n",
            "0.9 sock https://storage.googleapis.com/quickdraw_dataset/full/binary/sock.bin\n",
            "0.91 ice_cream https://storage.googleapis.com/quickdraw_dataset/full/binary/ice%20cream.bin\n",
            "0.92 moustache https://storage.googleapis.com/quickdraw_dataset/full/binary/moustache.bin\n",
            "0.93 suitcase https://storage.googleapis.com/quickdraw_dataset/full/binary/suitcase.bin\n",
            "0.94 hammer https://storage.googleapis.com/quickdraw_dataset/full/binary/hammer.bin\n",
            "0.95 rainbow https://storage.googleapis.com/quickdraw_dataset/full/binary/rainbow.bin\n",
            "0.96 knife https://storage.googleapis.com/quickdraw_dataset/full/binary/knife.bin\n",
            "0.97 cookie https://storage.googleapis.com/quickdraw_dataset/full/binary/cookie.bin\n",
            "0.98 baseball https://storage.googleapis.com/quickdraw_dataset/full/binary/baseball.bin\n",
            "0.99 lightning https://storage.googleapis.com/quickdraw_dataset/full/binary/lightning.bin\n",
            "1.0 bicycle https://storage.googleapis.com/quickdraw_dataset/full/binary/bicycle.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB0d6bjaC8mD",
        "colab_type": "code",
        "outputId": "741672bb-f10a-48f1-a1aa-2084acb96d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "print(len(os.listdir('data')))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBUTUh0MFI11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StrokeClassifier(nn.Module):\n",
        "  def __init__(self, cv1, cv2, hidden_dim, n_layers, n_classes, bidirectional):\n",
        "    super(StrokeClassifier, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    input_size = 3\n",
        "\n",
        "    self.bn = nn.BatchNorm1d(input_size)\n",
        "\n",
        "    if cv1 is not None:\n",
        "      self.conv1 = nn.Conv1d(input_size, cv1[0], cv1[1])\n",
        "      input_size = cv1[0]\n",
        "\n",
        "      if cv2 is not None:\n",
        "        self.conv2 = nn.Conv1d(input_size, cv2[0], cv2[1])\n",
        "        input_size = cv2[0]\n",
        "      else:\n",
        "        self.conv2 = None\n",
        "    else:\n",
        "      self.conv1 = None\n",
        "      self.conv2 = None\n",
        "    \n",
        "    # The LSTM takes 3 things as input (x, y, isLastPoint) and outputs hidden states with dimensionality hidden_dim\n",
        "    self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "    # The linear layer maps the LSTM output to a linear space\n",
        "    self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, n_classes)\n",
        "\n",
        "  def forward(self, strokes):\n",
        "    # BN and Convolution expect NCWH\n",
        "    strokes = self.bn(strokes)\n",
        "    if self.conv1 is not None:\n",
        "      strokes = self.conv1(strokes)\n",
        "\n",
        "      if self.conv2 is not None:\n",
        "        strokes = self.conv2(strokes)\n",
        "\n",
        "    # LSTM expect NHWC\n",
        "    strokes = torch.transpose(strokes, 1, 2)\n",
        "    out, _ = self.lstm(strokes)\n",
        "\n",
        "    # Keep last layer of the NN\n",
        "    out = out[:,-1,:]\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-bafz08v3BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DrawDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        assert len(self.X) == len(self.Y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        return (self.X[idx], self.Y[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQge5B5H-pKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Config:\n",
        "batch_size = 256\n",
        "learning_rate = 0.001\n",
        "\n",
        "hidden_size = 64\n",
        "n_layers = 1\n",
        "train_classes = classes[:]\n",
        "\n",
        "# Use None instead of (n_filters, filter_size) to disable convolution\n",
        "# Note that conv1 = None forces conv2 = None automatically\n",
        "conv1 = (128, 5)\n",
        "conv2 = None\n",
        "bidirectional = False\n",
        "\n",
        "N_train = 20000\n",
        "N_val = N_train // 5\n",
        "N_test = N_val\n",
        "N_test_reserved = 20000\n",
        "max_padding = 100\n",
        "n_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJPBw8l_WLvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import islice\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def extract_dataset(samples_train, samples_val, samples_test, test_reserved, classes, max_padding=100):\n",
        "  X_train = []\n",
        "  X_val = []\n",
        "  X_test = []\n",
        "  y_train = []\n",
        "  y_val = []\n",
        "  y_test = []\n",
        "\n",
        "  for c, cls in enumerate(classes):\n",
        "    drawings = unpack_drawings('data/' + cls + '.bin', max_padding)\n",
        "\n",
        "    # TODO: better way of doing this\n",
        "    for _ in range(samples_train):\n",
        "      X_train.append(next(drawings))\n",
        "      y_train.append(c)\n",
        "\n",
        "    # TODO: itertools\n",
        "    for _ in range(max(0, test_reserved - samples_train)):\n",
        "      next(drawings)\n",
        "\n",
        "    for _ in range(samples_val):\n",
        "      X_val.append(next(drawings))\n",
        "      y_val.append(c)\n",
        "\n",
        "    for _ in range(samples_test):\n",
        "      X_test.append(next(drawings))\n",
        "      y_test.append(c)\n",
        "  \n",
        "    \n",
        "    print(f\"\\rdone extracting class: {cls}: {1 + c} / {len(classes)}\", end='')\n",
        "\n",
        "    drawings.close()\n",
        "    \n",
        "\n",
        "  def norm(X):\n",
        "    return torch.FloatTensor(torch.transpose(pad_sequence(X, batch_first=True), 1, 2))\n",
        "\n",
        "  # XXX: instead of padding like that we could have a moving window:\n",
        "  # Example if we want 100 sequences and we have an image with 200 we can use the windows:\n",
        "  # 0-100, 10-110, ... 100-200 for instance, this would add data\n",
        "  X_train = norm(X_train)\n",
        "  X_val = norm(X_val)\n",
        "  X_test = norm(X_test)\n",
        "  #print(\"training shape\", X_train.shape)\n",
        "  print(\"validation shape\", X_val.shape)\n",
        "  print(\"testing shape\", X_test.shape)\n",
        "  print(\"classes\", len(classes))\n",
        "\n",
        "  return (\n",
        "      DrawDataset(X_train, torch.LongTensor(y_train)), \n",
        "      DrawDataset(X_val, torch.LongTensor(y_val)),\n",
        "      DrawDataset(X_test, torch.LongTensor(y_test)),\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jp5Zo6WcN2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, loader):\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for i, (img, label) in enumerate(loader):\n",
        "      img = img.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      out = model(img)\n",
        "\n",
        "      _, pred = torch.max(out.data, 1)\n",
        "\n",
        "      total += label.size(0)\n",
        "      correct += (pred == label).sum().item()\n",
        "\n",
        "    return 100. * correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eysyPVD42XAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training(losses, accs, n_epochs):\n",
        "  fig, ax1 = plt.subplots()\n",
        "\n",
        "  color = 'tab:red'\n",
        "  ax1.set_xlabel('epoch')\n",
        "  ax1.set_ylabel('training loss', color=color)\n",
        "  ax1.plot(losses, color=color)\n",
        "  ax1.tick_params(axis='y', labelcolor=color)\n",
        "  ax1.set_ylim([0, 5])\n",
        "\n",
        "  ax1.set_xlim([0, n_epochs])\n",
        "  ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "  color = 'tab:blue'\n",
        "  ax2.set_ylabel('validation accuracy', color=color)  # we already handled the x-label with ax1\n",
        "  ax2.plot(accs, color=color)\n",
        "  ax2.tick_params(axis='y', labelcolor=color)\n",
        "  ax2.set_ylim([0, 100])\n",
        "\n",
        "  fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Aua8_l2igl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "def train_model(model, opt, loss_fn, loader, v_loader, n_epochs):\n",
        "\n",
        "  best_acc, best_model = 0, None\n",
        "  losses, accs = [], []\n",
        "  for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    epoch_losses = []\n",
        "    for i, (img, lab) in enumerate(loader):\n",
        "      print(f\"\\rbatch: {i}, current loss: {np.mean(epoch_losses) if epoch_losses else 'NaN'}\", end='')\n",
        "      img = img.to(device)\n",
        "      lab = lab.to(device)\n",
        "\n",
        "      out = model(img)\n",
        "\n",
        "      loss = loss_fn(out, lab)\n",
        "\n",
        "      opt.zero_grad()\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      epoch_losses.append(loss.item())\n",
        "\n",
        "    print(\"\\rEvaluating model on validation dataset...\", end='')\n",
        "    val_acc = evaluate_model(model, v_loader)\n",
        "    mean_loss = np.mean(epoch_losses)\n",
        "\n",
        "    losses.append(mean_loss)\n",
        "    accs.append(val_acc)\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "      best_acc = val_acc\n",
        "      best_model = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model, f\"lstm_epoch_{epoch}_acc_{val_acc}.model\")\n",
        "\n",
        "    print(f\"\\rEpoch: {epoch+1}/{n_epochs}, loss: {mean_loss}, validation accuracy: {val_acc}% took: {time.time() - start} seconds\")\n",
        "\n",
        "  print(f\"Training ended after {n_epochs} ! Best validation accuracy: {best_acc}%\")\n",
        "  try:\n",
        "    plot_training(losses, accs, n_epochs)\n",
        "  except:\n",
        "    print(\"error occurred when plotting losses and accuracy training data\")\n",
        "  return best_model, losses, accs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaXyrAEfvap6",
        "colab_type": "code",
        "outputId": "c90fde70-dbea-4cae-9573-fc40dc9bb1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# TODO: really take the last 2k images for testing\n",
        "train_dataset, val_dataset, test_dataset = extract_dataset(N_train, N_val, N_test, N_test_reserved, train_classes, max_padding=max_padding)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done extracting class: drums: 1 / 100\n",
            "done extracting class: traffic_light: 11 / 100\n",
            "done extracting class: face: 21 / 100\n",
            "done extracting class: clock: 31 / 100\n",
            "done extracting class: diving_board: 41 / 100\n",
            "done extracting class: hat: 51 / 100\n",
            "done extracting class: basketball: 61 / 100\n",
            "done extracting class: pencil: 71 / 100\n",
            "done extracting class: donut: 81 / 100\n",
            "done extracting class: ice_cream: 91 / 100\n",
            "validation shape torch.Size([400000, 3, 100])\n",
            "testing shape torch.Size([400000, 3, 100])\n",
            "classes 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEGoqxR1y-6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaS_SobxWokC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = StrokeClassifier(conv1, conv2, hidden_size, n_layers, len(train_classes), bidirectional).to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F2SK1L5YWOQ",
        "colab_type": "code",
        "outputId": "fc9bf1f1-6a01-4401-cec0-0691f4d0b426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "best_model, losses, accs = train_model(model, optimizer, loss_function, train_loader, val_loader, n_epochs)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10, loss: 2.4521238011677897, validation accuracy: 42.35875% took: 140.67531204223633 seconds\n",
            "Epoch: 2/10, loss: 1.480470366893284, validation accuracy: 49.781% took: 139.75940108299255 seconds\n",
            "Epoch: 3/10, loss: 1.2990560588451563, validation accuracy: 51.917% took: 139.11033821105957 seconds\n",
            "Epoch: 4/10, loss: 1.2144290166857543, validation accuracy: 54.22025% took: 139.14547657966614 seconds\n",
            "Epoch: 5/10, loss: 1.161270840165253, validation accuracy: 56.294% took: 138.57833814620972 seconds\n",
            "Epoch: 6/10, loss: 1.12227923897093, validation accuracy: 56.4285% took: 138.07167291641235 seconds\n",
            "Epoch: 7/10, loss: 1.0923172702911979, validation accuracy: 57.21075% took: 138.1560935974121 seconds\n",
            "Epoch: 8/10, loss: 1.0696373967985238, validation accuracy: 57.8435% took: 140.04968333244324 seconds\n",
            "Epoch: 9/10, loss: 1.049920146655719, validation accuracy: 58.2105% took: 138.90221500396729 seconds\n",
            "Epoch: 10/10, loss: 1.0328531821558635, validation accuracy: 59.2585% took: 138.53191256523132 seconds\n",
            "Training ended after 10 ! Best validation accuracy: 59.2585%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCSS/j+GoXRQSc0uNQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bynKPlEifCuI",
        "colab_type": "code",
        "outputId": "ed20a7ce-e463-4126-e739-1eca07ac68ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"Test accuracy: {evaluate_model(model, test_loader)}%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 79.715%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaCHS8rMi7DK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model_path = f'lstm_quickdraw.model.{time.time()}'\n",
        "torch.save(best_model, model_path)\n",
        "\n",
        "print(f\"Model saved at: {model_path}\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}